### Introduction

This projects uses the accelerometer measurement of 6 people over the time. The data contains the accelerometer measurement mesurement for different type of acctivities and label identifying the quality of the activity.The goal of the project is to create the prediction model to predict the label for the test data sets given.

The project describes each steps taken to build the model and all the preprocessing done in data sets to reach the mode.

### Data Preprocessing and Preparation 

The ```caret``` package is used for this project.Training and test data sets are read and test data is not used until the model is built.

```{r}
library(caret)
ptrain <- read.csv("data/pml-training.csv")
ptest <- read.csv("data/pml-testing.csv")
```

To estimate the *out-of-sample* error, the training set ```ptrain```  is splitted into training and validation set: ```ptrain1``` and ```validation```.  

```{r}
set.seed(1000)
inTrain <- createDataPartition(y = ptrain$classe, p = 0.7, list = FALSE)
ptrain1 <- ptrain[inTrain, ]
validation <- ptrain[-inTrain, ]
```

We will now anlyse the each features of the ```ptrain1``` and remove feauters which do not contribute much to the final model.The features with almost *zero and NA values* are not useful for building model.Also, the feature having almost *zero variance* do not contribute.Other feature variable such as name do not contribute the model but make the prediction model more complex.Now we will analyze the ```ptrain1``` and remove those features and we will apply same to the ```validation``` set also.

```{r}
# remove features with nearly with zero variance using 
nzVar <-  nearZeroVar(ptrain1)
ptrain1 <- ptrain1[, -nzVar]
# Applying same to validation set
validation <- validation[, -nzVar]

# check for features with almost NAs value and remove if any
almostNAs <- sapply(ptrain1 , function(x) mean(is.na(x))) > 0.95
ptrain1 <- ptrain1[, almostNAs == F]
validation <- validation[, almostNAs == F]

# remove variables which are not relevant for building the model. The first 5 feature varible * X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp* are not relevant.
ptrain1 <- ptrain1[, -(1:5)]
validation <- validation[, -(1:5)]
```

### Model Construction

First, I planned to build the model using ```neural network```.Since the problem is supervised learning and neural nets are good at classification task.To make the model more robust I will use 3 - fold cross validation to build the model.

```{r}
# Use 3-fold cross validation to build the model
controlPara <- trainControl(method = "cv", number = 3, verboseIter = F,)

#build model using ptrain1

modelFit1 <- train(classe ~ ., data = ptrain1, method = "nnet", trControl = controlPara,trace = F)
```


### Model Evaluation
The fitted model is used to predict tje label for our validation set and find the *out-of-sample* error.Confusion matrix is used to compare the output from the our model to the actual labels of the data.

```{r}
prediction <- predict(modelFit1, newdata = validation)
confusionMatrix(validation$classe, prediction)
```

The performance of this model is less than satisfactory with just 38 % accuracy.The matrix shows there are lots of misclassification. The nnet is feed forward model so the neural network with back propagation may give more accuracy then just feed forward model. 

#### Second Model Construction

Let's try to build the second model with ```Random Forest``` and evaluate the accuracy of model on the validation data sets.The model is build with 3-fold CV.

```{r}
# Use 3-fold cross validation to build the model
controlPara <- trainControl(method = "cv", number = 3, verboseIter = F,)

modelFit2 <- train(classe ~. , data = ptrain1 , trControl = controlPara )
```


